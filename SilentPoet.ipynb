{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SILENT POET\n",
    "\n",
    "This project aims to generate Shakeshpere style pargraphs using GANS. The GAN will try to generate a random pargraph which can later be improved to take some basic requirements like story line in the input itself to generate paragraph with those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preparing The Dataset\n",
    "1. Get the NLTk corpus\n",
    "2. Join the Sentences to make a string\n",
    "3. Combine Multiple Sentences to Make a Pragraph fo desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.corpus as corpus\n",
    "g_sents=corpus.gutenberg.sents()\n",
    "g_sents=[\" \".join(i) for i in g_sents]\n",
    "data=[\".\".join(g_sents[i:i+25]) for i in range(len(g_sents)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "with open('word_index.pickle','rb') as word_index_file:\n",
    "    word_index=pk.load(word_index_file)\n",
    "tokenizer=Tokenizer(lower=True,filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.num_words=10000\n",
    "tokenizer.fit_on_texts([\" \".join(list(word_index.keys()))])\n",
    "sequences=tokenizer.texts_to_sequences(data)\n",
    "word_index=tokenizer.word_index\n",
    "data=pad_sequences(sequences,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings_pickle.pickle','rb') as embeddings:\n",
    "     embeddings_index=pk.load(embeddings)\n",
    "EMBEDDING_DIM = 50\n",
    "num_words = len(word_index.keys())\n",
    "embedding_matrix = np.zeros((num_words+1, EMBEDDING_DIM))\n",
    "embeddings_index={k.lower():embeddings_index[k] for k in embeddings_index}\n",
    "for word, i in word_index.items():    \n",
    "    embedding_vector = embeddings_index.get(word.lower())\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "del embeddings_index,sequences,word_index,g_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embedded_inputs(data):\n",
    "    return np.reshape(embedding_matrix[data[:,:]],[-1,500,50,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE=10\n",
    "NOISE_SHAPE=[16,16]\n",
    "init = tf.global_variables_initializer()\n",
    "def generateRandomShape(length=256,batch_size=5000):    \n",
    "    inp=np.random.normal(0,1,size=[batch_size,length])\n",
    "    return inp\n",
    "\n",
    "def build_generator(inp,batch_size=32,reuse=False):\n",
    "    with tf.variable_scope('gen',reuse=False) as scope:\n",
    "        if(reuse):\n",
    "            scope.reuse_variables()\n",
    "        inp_r=tf.reshape(inp,shape=[batch_size,NOISE_SHAPE[0],NOISE_SHAPE[1],1])\n",
    "\n",
    "        #conv2d layer 1\n",
    "        g_l1=tf.layers.conv2d(inp_r,filters=16,kernel_size=(1,1),strides=(1,1),padding=\"SAME\",name=\"g_l1\")\n",
    "        g_up1=tf.image.resize_images(g_l1,size=(50,18),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "        #conv2d layer 2\n",
    "        g_l2=tf.layers.conv2d(g_up1,filters=32,kernel_size=(2,2),strides=(1,1),padding=\"SAME\",name=\"g_l2\")\n",
    "        g_up2=tf.image.resize_images(g_l2,size=(100,20),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "        #conv2d layer 3\n",
    "        g_l3=tf.layers.conv2d(g_up2,filters=16,kernel_size=(3,3),strides=(1,1),padding=\"SAME\",name=\"g_l3\")\n",
    "        g_up3=tf.image.resize_images(g_l3,size=(250,25),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "        #conv2d layer 4\n",
    "        g_l4=tf.layers.conv2d(g_up3,filters=1,kernel_size=(3,3),strides=(1,1),padding=\"SAME\",name=\"g_l4\")\n",
    "        g_up4=tf.image.resize_images(g_l4,size=(500,50),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "        return g_up4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Descriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_descriminator(inputs,reuse=False):\n",
    "    #conv1\n",
    "    with tf.variable_scope('dis',reuse=False) as scope:\n",
    "        if(reuse):\n",
    "            scope.reuse_variables()\n",
    "        d_1=tf.layers.conv2d(inputs,filters=32,kernel_size=(3,3),padding=\"SAME\",name=\"d_l1\")     \n",
    "        d_2=tf.layers.max_pooling2d(d_1,pool_size=(2,2),strides=(1,1),name=\"d_mp1\")\n",
    "        d_3=tf.layers.conv2d(d_2,filters=16,kernel_size=(3,3),padding=\"SAME\",name=\"d_l2\")\n",
    "        d_4=tf.layers.max_pooling2d(d_3,pool_size=(2,1),strides=(1,1),name=\"d_mp2\")\n",
    "        d_5=tf.layers.conv2d(d_4,filters=16,kernel_size=(3,3),padding=\"SAME\",name=\"d_l3\")\n",
    "        d_6=tf.layers.max_pooling2d(d_5,pool_size=(2,2),strides=(1,1),name=\"d_mp3\")\n",
    "        d_7=tf.layers.flatten(d_6)\n",
    "        d_8=tf.layers.dropout(d_7,rate=0.5)\n",
    "        d_9=tf.layers.dense(d_8,10,name=\"d_d2\",reuse=tf.AUTO_REUSE)\n",
    "        d_10=tf.layers.dense(d_9,2,name=\"d_d3\",reuse=tf.AUTO_REUSE)\n",
    "        d_11=tf.nn.sigmoid(d_10)\n",
    "        return d_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=generateRandomShape(batch_size=BATCH_SIZE)\n",
    "x_=tf.placeholder(dtype=tf.float32,shape=(None,256))\n",
    "d_x_=tf.placeholder(shape=[None,500,50,1],dtype=tf.float32)  \n",
    "a=build_generator(x_,batch_size=BATCH_SIZE)\n",
    "d_=build_descriminator(d_x_)\n",
    "with tf.Session() as sess:\n",
    "    init=tf.global_variables_initializer()\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    sess.run(init)\n",
    "    generated_output=sess.run(a,{x_:train_x})  \n",
    "    print(sess.run(d_,feed_dict={d_x_:generated_output}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Losses for Generator and Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dis/d_l1/kernel:0', 'dis/d_l1/bias:0', 'dis/d_l2/kernel:0', 'dis/d_l2/bias:0', 'dis/d_l3/kernel:0', 'dis/d_l3/bias:0', 'dis/d_d2/kernel:0', 'dis/d_d2/bias:0', 'dis/d_d3/kernel:0', 'dis/d_d3/bias:0']\n",
      "['gen/g_l1/kernel:0', 'gen/g_l1/bias:0', 'gen/g_l2/kernel:0', 'gen/g_l2/bias:0', 'gen/g_l3/kernel:0', 'gen/g_l3/bias:0', 'gen/g_l4/kernel:0', 'gen/g_l4/bias:0']\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "tf.reset_default_graph()\n",
    "o_x_=get_embedded_inputs(data[:1000,:])\n",
    "x_placeholder=tf.placeholder(shape=[None,500,50,1],dtype=tf.float32)\n",
    "z_placeholder=tf.placeholder(shape=[None,256],dtype=tf.float32)\n",
    "Dx=build_descriminator(x_placeholder)\n",
    "Gz=build_generator(z_placeholder,batch_size=batch_size)\n",
    "Dg=build_descriminator(Gz,reuse=True)\n",
    "#d_o_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(Dx, tf.ones_like(Dx)))\n",
    "\n",
    "d_o_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dx, labels=tf.ones_like(Dx)))\n",
    "d_f_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg,labels=tf.zeros_like(Dg)))\n",
    "g_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Dg,labels=tf.ones_like(Dg)))\n",
    "tvars = tf.trainable_variables()\n",
    "sess=tf.Session()\n",
    "\n",
    "d_vars = [var for var in tvars if 'dis' in var.name]\n",
    "g_vars = [var for var in tvars if 'gen' in var.name]\n",
    "\n",
    "print([v.name for v in d_vars])\n",
    "print([v.name for v in g_vars])\n",
    "\n",
    "d_trainer_fake = tf.train.AdamOptimizer(0.0003).minimize(d_f_loss, var_list=d_vars)\n",
    "d_trainer_real = tf.train.AdamOptimizer(0.0003).minimize(d_o_loss, var_list=d_vars)\n",
    "\n",
    "# Train the generator\n",
    "g_trainer = tf.train.AdamOptimizer(0.0001).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "tf.summary.scalar('Generator_loss', g_loss)\n",
    "tf.summary.scalar('Discriminator_loss_real', d_o_loss)\n",
    "tf.summary.scalar('Discriminator_loss_fake', d_f_loss)\n",
    "\n",
    "images_for_tensorboard = build_generator(z_placeholder,reuse=True)\n",
    "tf.summary.image('Generated_images', images_for_tensorboard, 5)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"SilentPoet/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLossReal: 0.497493 dLossFake: 0.957878\n",
      "dLossReal: 0.313262 dLossFake: 0.693147\n",
      "dLossReal: 0.313262 dLossFake: 0.693148\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "index=0\n",
    "for i in range(300):\n",
    "    if((index+1)*10>data.shape[0]):\n",
    "        index=0\n",
    "    z_batch=generateRandomShape(batch_size=batch_size,length=256)\n",
    "    real_text_batch=get_embedded_inputs(data[index*10:(index+1)*10])\n",
    "    _, __, dLossReal, dLossFake = sess.run([d_trainer_real, d_trainer_fake, d_o_loss, d_f_loss],feed_dict={\n",
    "        x_placeholder:real_text_batch,z_placeholder:z_batch\n",
    "    })\n",
    "    if(i % 100 == 0):\n",
    "        print(\"dLossReal:\", dLossReal, \"dLossFake:\", dLossFake)\n",
    "    index+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 at 2018-06-13 21:38:11.522762\n",
      "Iteration: 100 at 2018-06-13 21:43:17.737762\n"
     ]
    }
   ],
   "source": [
    "index=0\n",
    "for i in range(100000):\n",
    "    #train discriminator\n",
    "    z_batch=generateRandomShape(batch_size=batch_size,length=256)\n",
    "    \n",
    "    if((index+1)*10>data.shape[0]):\n",
    "        index=0\n",
    "    real_text_batch=get_embedded_inputs(data[index*10:(index+1)*10])\n",
    "    _, __, dLossReal, dLossFake = sess.run([d_trainer_real, d_trainer_fake, d_o_loss, d_f_loss],feed_dict={\n",
    "        x_placeholder:real_text_batch,z_placeholder:z_batch\n",
    "    })\n",
    "    #train generator\n",
    "    z_batch=generateRandomShape(batch_size=batch_size,length=256)\n",
    "    _ = sess.run(g_trainer, feed_dict={z_placeholder: z_batch})\n",
    "    if i % 10 == 0:\n",
    "        # Update TensorBoard with summary statistics\n",
    "        z_batch = generateRandomShape(batch_size=batch_size,length=256)\n",
    "        summary = sess.run(merged, {z_placeholder: z_batch, x_placeholder: real_text_batch})\n",
    "        writer.add_summary(summary, i)\n",
    "    if i % 100 == 0:\n",
    "        # Every 100 iterations, show a generated image\n",
    "        print(\"Iteration:\", i, \"at\", datetime.datetime.now())\n",
    "        z_batch = generateRandomShape(batch_size=batch_size,length=256)\n",
    "        generated_images = build_generator(z_placeholder, 1,reuse=True)\n",
    "    index+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
