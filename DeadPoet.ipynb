{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEAD POET\n",
    "\n",
    "This project aims to generate Shakeshpere style pargraphs using GANS. The GAN will try to generate a random pargraph which can later be improved to take some basic requirements like story line in the input itself to generate paragraph with those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preparing The Dataset\n",
    "1. Get the NLTk corpus\n",
    "2. Join the Sentences to make a string\n",
    "3. Combine Multiple Sentences to Make a Pragraph fo desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.corpus as corpus\n",
    "g_sents=corpus.gutenberg.sents()\n",
    "g_sents=[\" \".join(i) for i in g_sents]\n",
    "data=[\".\".join(g_sents[i:i+25]) for i in range(len(g_sents)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "with open('word_index.pickle','rb') as word_index_file:\n",
    "    word_index=pk.load(word_index_file)\n",
    "tokenizer=Tokenizer(lower=True,filters='!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.num_words=10000\n",
    "tokenizer.fit_on_texts([\" \".join(list(word_index.keys()))])\n",
    "sequences=tokenizer.texts_to_sequences(data)\n",
    "word_index=tokenizer.word_index\n",
    "data=pad_sequences(sequences,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 500, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "BATCH_SIZE=10\n",
    "NOISE_SHAPE=[16,16]\n",
    "init = tf.global_variables_initializer()\n",
    "def generateRandomShape(length=256,batch_size=5000):\n",
    "    \n",
    "    inp=np.random.rand(batch_size,length)\n",
    "    return inp\n",
    "\n",
    "def build_generator(inp,batch_size):\n",
    "    inp=tf.reshape(inp,shape=[batch_size,NOISE_SHAPE[0],NOISE_SHAPE[1],1])\n",
    "    \n",
    "    #conv2d layer 1\n",
    "    g_l1=tf.layers.conv2d(inp,filters=16,kernel_size=(1,1),strides=(1,1),padding=\"SAME\",name=\"g_l1\")\n",
    "    g_up1=tf.image.resize_images(g_l1,size=(50,18),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    #conv2d layer 2\n",
    "    g_l2=tf.layers.conv2d(g_up1,filters=32,kernel_size=(2,2),strides=(1,1),padding=\"SAME\",name=\"g_l2\")\n",
    "    g_up2=tf.image.resize_images(g_l2,size=(100,20),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    #conv2d layer 3\n",
    "    g_l3=tf.layers.conv2d(g_up2,filters=16,kernel_size=(3,3),strides=(1,1),padding=\"SAME\",name=\"g_l3\")\n",
    "    g_up3=tf.image.resize_images(g_l3,size=(250,25),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    #conv2d layer 4\n",
    "    g_l4=tf.layers.conv2d(g_up3,filters=1,kernel_size=(3,3),strides=(1,1),padding=\"SAME\",name=\"g_l4\")\n",
    "    g_up4=tf.image.resize_images(g_l4,size=(500,50),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    return g_up4\n",
    "\n",
    "train_x=generateRandomShape(batch_size=BATCH_SIZE)\n",
    "x_=tf.placeholder(dtype=tf.float32,shape=(None,256))\n",
    "a=build_generator(x_,batch_size=BATCH_SIZE)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init=tf.global_variables_initializer()\n",
    "    tf.get_variable_scope().reuse_variables()\n",
    "    sess.run(init)\n",
    "    print(sess.run(a,{x_:train_x}).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Descriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_descriminator():\n",
    "    #conv1\n",
    "    inps=tf.placeholder(dtype=tf.float64,shape=(None,None,50,1))\n",
    "    d_=tf.layers.conv2d(inps,filters=32,kernel_size=(3,3),strides=1,name=\"d_l1\")\n",
    "    d_=tf.max_pooling2d(d_,pool_size=(2,2),name=\"d_mp1\")\n",
    "    d_=tf.layers.conv2d(inps,filters=16,kernel_size=(3,3),strides=1,name=\"d_l1\")\n",
    "    d_=tf.max_pooling2d(d_,pool_size=(2,2),name=\"d_mp1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
